<!DOCTYPE html>
<html lang="zh-CN">
  <head>
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width,initial-scale=1">
    <title>模式识别（本科生专业选修课）知识点整理 | 梦飞翔的小站</title>
    <meta name="description" content="A VitePress site">
    <link rel="stylesheet" href="/assets/style.0565aa79.css">
    <link rel="modulepreload" href="/assets/app.d81b9fb5.js">
    <link rel="modulepreload" href="/assets/posts_pattern-recognition-note.md.f88ce139.lean.js">
    
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@6.0.0/css/regular.min.css">
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@6.0.0/css/all.min.css">
  <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Noto+Serif+SC">
  <script src="https://cdn.jsdelivr.net/npm/@waline/client@1.5.4/dist/Waline.min.js"></script>
  <script src="https://cdn.jsdelivr.net/npm/katex@0.15.2/dist/katex.min.js"></script>
  <script src="https://cdn.jsdelivr.net/npm/katex@0.15.2/dist/contrib/auto-render.min.js"></script>
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.15.2/dist/katex.min.css">
  <meta name="twitter:title" content="模式识别（本科生专业选修课）知识点整理 | 梦飞翔的小站">
  <meta property="og:title" content="模式识别（本科生专业选修课）知识点整理 | 梦飞翔的小站">
  </head>
  <body>
    <div id="app"><!--[--><header><span class="brand"></span><span class="container"><span class="nav"></span><span class="menu"><ul><!--[--><li><a href="/"><span><i class="fa fa-home"></i> 首页</span></a></li><li><a href="/tags/"><span><i class="fa fa-tag"></i> 标签</span></a></li><li><a href="/readme.html"><span><i class="fa fa-leaf"></i> 关于</span></a></li><!--]--></ul></span></span><span><span class="search"><i class="fa fa-search"></i></span></span></header><aside></aside><main><a href="#" class="totop" style="top:-900px;" aria-label="to-top"></a><!--[--><div class="abanner" style="background-image: url(https://tva4.sinaimg.cn/large/0060lm7Tly1ftg6omnqa4j31hc0u010z.jpg)"><div class="titlebox"><h1 class="title">模式识别（本科生专业选修课）知识点整理</h1><div class="info">flaribbit · 更新于 Invalid Date · 0 次阅读</div></div></div><div class="article"><div style="position:relative;" class="content"><div><p>备份一下之前的笔记</p><hr><h2 id="第三章-判别域代数界面方程" tabindex="-1">第三章 判别域代数界面方程 <a class="header-anchor" href="#第三章-判别域代数界面方程" aria-hidden="true">#</a></h2><h3 id="_3-1-用判别域界面方程分类的概念" tabindex="-1">3.1 用判别域界面方程分类的概念 <a class="header-anchor" href="#_3-1-用判别域界面方程分类的概念" aria-hidden="true">#</a></h3><h4 id="_1-分类的基本原理" tabindex="-1">1.分类的基本原理 <a class="header-anchor" href="#_1-分类的基本原理" aria-hidden="true">#</a></h4><p>不用模式对应特征点在不同区域中散布。运用已知类别的训练样本进行学习，产生若干个代数界面$d(\vec x)=0$，将特征空间划分成一些互不重叠的子区域。</p><h4 id="_2-判别函数" tabindex="-1">2.判别函数 <a class="header-anchor" href="#_2-判别函数" aria-hidden="true">#</a></h4><p>表示划分界面的函数。</p><h4 id="_3-线性可分的定义" tabindex="-1">3.线性可分的定义 <a class="header-anchor" href="#_3-线性可分的定义" aria-hidden="true">#</a></h4><p>对于来自两类的一组模式$\vec x_1,\vec x_2,\dots,\vec x_N$，如果能用一个线性判别函数正确分类，则称他们是线性可分的。</p><h4 id="_4-分类方法的基本技术思路" tabindex="-1">4.分类方法的基本技术思路 <a class="header-anchor" href="#_4-分类方法的基本技术思路" aria-hidden="true">#</a></h4><ol><li>利用训练样本求出分类器/判别函数</li><li>利用判别函数对未知类别样本分类</li></ol><h3 id="_3-2-线性判别函数" tabindex="-1">3.2 线性判别函数 <a class="header-anchor" href="#_3-2-线性判别函数" aria-hidden="true">#</a></h3><p>一般形式是$d(\vec x)=w_1x_1+w_2x_2+\dots+w_nx_n+w_{n+1}$</p><p>$\vec w$称为权矢量或系数矢量</p><p>简化为$d(\vec x)=\vec w&#39;\vec x$</p><p>其中$\vec x=(x_1,x_2,\dots,x_n,1),\vec w=(w_1,w_2,\dots,w_n,w_{n+1})$</p><p>$\vec x$和$\vec w$分别称为增广特征矢量和增广权矢量。</p><h4 id="两类问题" tabindex="-1">两类问题 <a class="header-anchor" href="#两类问题" aria-hidden="true">#</a></h4><p>对于两类问题</p><p>$d(\vec x)=\vec w&#39;\vec x\begin{cases} &gt;0\Rightarrow\vec x\in\omega_1\\ &lt;0\Rightarrow\vec x\in\omega_2\\ =0\Rightarrow\vec x\in\omega_i或拒判\\ \end{cases}$</p><h4 id="多类问题" tabindex="-1">多类问题 <a class="header-anchor" href="#多类问题" aria-hidden="true">#</a></h4><h5 id="_1-两分法（第一种情况）" tabindex="-1">1.$\omega_i/\bar\omega_i$两分法（第一种情况） <a class="header-anchor" href="#_1-两分法（第一种情况）" aria-hidden="true">#</a></h5><p>判别规则为：如果$\begin{cases} d_i(\vec x)&gt;0\\ d_j(\vec x)\leqslant0&amp;\forall j\ne i \end{cases}$则判$\vec x\in\omega_i$</p><p>注意这种方法存在不确定区域</p><h5 id="_2-两分法（第二种情况）" tabindex="-1">2.$\omega_i/\omega_j$两分法（第二种情况） <a class="header-anchor" href="#_2-两分法（第二种情况）" aria-hidden="true">#</a></h5><p>对于任意两类之间分别建立判别函数</p><p>判别规则为：如果$d_{ij}(x)&gt;0,\forall j\ne i$则判$\vec x\in\omega_i$</p><p>注意这种方法也存在不确定区域</p><h5 id="_3-没有不确定区域的两分法（第三种情况）" tabindex="-1">3.没有不确定区域的$\omega_i/\omega_j$两分法（第三种情况） <a class="header-anchor" href="#_3-没有不确定区域的两分法（第三种情况）" aria-hidden="true">#</a></h5><p>令方法2中的判别函数为$d_{ij}(\vec x)=d_i(\vec x)-d_j(\vec x)=(\vec\omega_i-\vec\omega_j)&#39;\vec x$</p><p>判别规则为：如果$d_i(\vec x)&gt;d_j(\vec x),\forall j\ne i$则判$\vec x\in\omega_i$</p><p>或者：如果$d_i(\vec x)=\max_j[d_j(\vec x)]$则判$\vec x\in\omega_i$</p><h4 id="小结" tabindex="-1">小结 <a class="header-anchor" href="#小结" aria-hidden="true">#</a></h4><ul><li>当$c&gt;3$时，$\omega_i/\omega_j$法比$\omega_i/\bar\omega_i$法需要更多的判别函数式。</li><li>$\omega_i/\omega_j$法使模式更容易线性可分</li><li>方法3判别函数数目与方法1相同，但没有不确定区，分析简单，是最常用的方法</li></ul><h3 id="_3-3-判别函数值的鉴别意义、权空间及解空间" tabindex="-1">3.3 判别函数值的鉴别意义、权空间及解空间 <a class="header-anchor" href="#_3-3-判别函数值的鉴别意义、权空间及解空间" aria-hidden="true">#</a></h3><h4 id="数学意义" tabindex="-1">数学意义 <a class="header-anchor" href="#数学意义" aria-hidden="true">#</a></h4><ul><li>系数矢量$\vec w=(w_1,w_2,\dots,w_n)$是该平面的法矢量。</li><li>判别函数$d(\vec x)$的绝对值正比于$\vec x$到超平面$d(\vec x)=0$的距离。</li><li>判别函数的正负表示出特征点位于哪个半空间中</li></ul><h4 id="权空间、解矢量、解空间" tabindex="-1">权空间、解矢量、解空间 <a class="header-anchor" href="#权空间、解矢量、解空间" aria-hidden="true">#</a></h4><ul><li>将权系数视为变量，则由其组成的增广权矢量的全体构成增广权空间。</li><li>系数矢量$\vec w$指向判别函数的正侧。</li><li>解矢量是能够正确分类的权矢量。</li><li>满足上面各不等式的$\vec w$必在该锥体中，即锥中每一点都是上面不等式组的解，解矢量不是唯一的，上述的凸多面锥包含了解的全体，称其为解区、解空间或解锥。</li></ul><h3 id="_3-4-fisher线性判别" tabindex="-1">3.4 Fisher线性判别 <a class="header-anchor" href="#_3-4-fisher线性判别" aria-hidden="true">#</a></h3><p>思想：通过Fisher变换转换为利于分类的一维问题</p><p>方法：求权矢量$\vec w\Rightarrow$求满足上述目标的投影轴方向$\vec w_0$和在一维空间中确定判别规则。</p><p>希望经过投影后，类内离差度越小越好，类间离差度越大越好，根据这个目标作准则函数（即Fisher准则函数），并使其最大。</p><h4 id="算法过于硬核，告辞" tabindex="-1">算法过于硬核，告辞 <a class="header-anchor" href="#算法过于硬核，告辞" aria-hidden="true">#</a></h4><h3 id="_3-5-感知器算法" tabindex="-1">3.5 感知器算法 <a class="header-anchor" href="#_3-5-感知器算法" aria-hidden="true">#</a></h3><h4 id="感知器算法" tabindex="-1">感知器算法 <a class="header-anchor" href="#感知器算法" aria-hidden="true">#</a></h4><p>算法原理步骤</p><ol><li>置步数$k=1$，令增量$\rho=\rho_0$，分别赋给初始增广权矢量$\vec w(1)$的各分量较小的任意值。</li><li>输入训练模式$\vec x_k$，计算判别函数值$\vec w&#39;(k)\vec x_k$</li><li>调整增广权矢量<br> 如果$\vec x_k\in\omega_1$和$\vec w&#39;(k)\leqslant 0$，则$\vec w(k+1)=\vec w(k)+\rho\vec x_k$<br> 如果$\vec x_k\in\omega_2$和$\vec w&#39;(k)\geqslant 0$，则$\vec w(k+1)=\vec w(k)-\rho\vec x_k$<br> 如果$\vec x_k\in\omega_1$和$\vec w&#39;(k)&gt;0$，或$\vec x_k\in\omega_2$和$\vec w&#39;(k)&lt;0$，则$\vec w(k+1)=\vec w(k)$</li><li>如果$k&lt;N$，令$k=k+1$，返回2。如果$k=N$，检验判别函数是否都能正确分类，如果是，结束，否则令$k=1$，返回2。</li></ol><h4 id="收敛定理" tabindex="-1">收敛定理 <a class="header-anchor" href="#收敛定理" aria-hidden="true">#</a></h4><p>如果训练模式是线性可分的，感知器算法在有限次迭代后便可以收敛到正确的解矢量。</p><h4 id="一次准则函数和梯度下降法" tabindex="-1">一次准则函数和梯度下降法 <a class="header-anchor" href="#一次准则函数和梯度下降法" aria-hidden="true">#</a></h4><ul><li>当$\rho_k$为常数时，梯度下降法的迭代公式和感知器算法是一致的。</li><li>当$\rho_k$取常数时，$\rho_k$小收敛慢，$\rho_k$大震荡。</li><li>改进方法时$\rho_k$随$k$变化，称为可变增量法。</li></ul><h4 id="感知器算法在多类问题中的应用" tabindex="-1">感知器算法在多类问题中的应用 <a class="header-anchor" href="#感知器算法在多类问题中的应用" aria-hidden="true">#</a></h4><p>不做要求</p><h3 id="_3-6-一般情况下的判别函数权矢量算法" tabindex="-1">3.6 一般情况下的判别函数权矢量算法 <a class="header-anchor" href="#_3-6-一般情况下的判别函数权矢量算法" aria-hidden="true">#</a></h3><p>（下面三个了解即可） 最小错分模式数目准则 分段二次准则函数 最小方差准则及W-H算法</p><h3 id="_3-7-广义线性判别函数" tabindex="-1">3.7 广义线性判别函数 <a class="header-anchor" href="#_3-7-广义线性判别函数" aria-hidden="true">#</a></h3><p>作非线性变换，将原来一维特征空间映射为二维特征空间，使其为线性可分的。</p><h3 id="_3-8-二次判别函数" tabindex="-1">3.8 二次判别函数 <a class="header-anchor" href="#_3-8-二次判别函数" aria-hidden="true">#</a></h3><p>$d(\vec x)=\vec x&#39;W\vec x+\vec w&#39;\vec x+w_{n+1}$</p><h3 id="_3-9-支持向量机" tabindex="-1">3.9 支持向量机 <a class="header-anchor" href="#_3-9-支持向量机" aria-hidden="true">#</a></h3><p>支持向量机以训练误差作为优化问题的约束条件，以置信范围值最优化作为优化目标，即SVM是一种基于结构风险最小化准则的学习方法，其推广能力明显优于一些传统的学习方法。</p><p>由于SVM的求解最后转化为二次规划问题求解，因此SVM的解是全局唯一的最优解。</p><p>SVM在解决小样本、非线性及高维模式识别问题中表现出许多特有的优势，并能够推广应用到函数拟合等许多机器学习问题中。</p><h4 id="svm方法的特点" tabindex="-1">SVM方法的特点 <a class="header-anchor" href="#svm方法的特点" aria-hidden="true">#</a></h4><ul><li>非线性映射是SVM方法的理论基础，SVM利用内积核函数代替向高维空间的非线性映射。</li><li>对特征空间划分的最优超平面是SVM的目标，最大化分类边际的思想是SVM方法和核心。</li><li>支持向量是SVM的训练结果，在SVM分类决策中起决定作用的是支持向量。</li><li>SVM的最终决策只由少数支持向量所确定，计算的复杂性取决于支持向量的数目，而不是样本空间的维数。</li><li>SVM的最终决策只由少数支持向量所确定，注定了该方法具有较好的鲁棒性 <ul><li>增删非支持向量样本对模型没有影响</li><li>支持向量样本集具有一定的鲁棒性</li><li>有些成功的应用中，SVM对核的选区不敏感</li></ul></li></ul><h2 id="第四章-统计判别" tabindex="-1">第四章 统计判别 <a class="header-anchor" href="#第四章-统计判别" aria-hidden="true">#</a></h2><h3 id="_4-0-前提条件" tabindex="-1">4.0 前提条件 <a class="header-anchor" href="#_4-0-前提条件" aria-hidden="true">#</a></h3><ol><li>各类别总体概率密度是已知的</li><li>要判决的类别数是一定的</li></ol><h3 id="_4-1-最小误判概率判决" tabindex="-1">4.1 最小误判概率判决 <a class="header-anchor" href="#_4-1-最小误判概率判决" aria-hidden="true">#</a></h3><p>判决规则：</p><p>如果：$l_{12}(\vec x)=\dfrac{p(\vec x|\omega_1)}{p(\vec x|\omega_2)}\gtrless\dfrac{P(\omega_2)}{P(\omega_1)}$，则判$\vec x\in\begin{cases} \omega_1\\ \omega_2\\ \end{cases}$</p><p>称$l_{12}(\vec x)$为似然比，称$\theta_{12}$为似然比阈值，记为$\dfrac{P(\omega_2)}{P(\omega_1)}$。</p><p>对于多类问题，若$P(\omega_i|\vec x)&gt;P(\omega_j|\vec x),\forall j\ne i$，则判$\vec x\in\omega_i$</p><p>或者，若$P(\omega_i|\vec x)=\max_j[P(\omega_j|\vec x)]$，则判$\vec x\in\omega_i$</p><hr><p>例题：对一批人进行癌症普查，患癌症者定为属$\omega_1$类，正常者定为属$\omega_2$类。统计资料表明人们患癌的概率$P(\omega_1)=0.005$，从而$P(\omega_2)=0.995$。设有一种诊断此病的试验，其结果有阳性反应和阴性反应之分，依其作诊断。化验结果是一维离散模式特征。统计资料表明：癌症者有阳性反映的概率为0.95即$P(x=阳|\omega_1)=0.95$，从而可知$P(x=阴|\omega_1)=0.05$，正常人阳性反映的概率为0.01即$P(x=阳|\omega_2)=0.01$, 可知$P(x=阴|\omega_2)=0.99$。</p><p>问有阳性反映的人患癌症的概率有多大？按照最小误判概率准则，阳性反映者应判为哪一类？</p><p>解：</p><p>$\begin{aligned} P(\omega_1|x=阳)&amp;=\dfrac{P(x=阳|\omega_1)P(\omega_1)}{P(x=阳)}\\ &amp;=\dfrac{P(x=阳|\omega_1)P(\omega_1)}{P(x=阳|\omega_1)P(\omega_1)+P(x=阳|\omega_2)P(\omega_2)}\\ &amp;=\dfrac{0.95\times0.005}{0.95\times0.005+0.01\times0.995}\\ &amp;=0.323 \end{aligned}$</p><p>$P(\omega_2|x=阳)=1-P(\omega_1|x=阳)=0.677$</p><p>所以$\vec x\in\omega_2$</p><p>或者似然比形式</p><p>$l_{12}(x)=\dfrac{P(x=阳|\omega_1)}{P(x=阳|\omega_2)}=\dfrac{0.95}{0.01}=95$</p><p>$\theta_{12}=\dfrac{P(\omega_2)}{P(\omega_1)}=\dfrac{0.995}{0.005}=197$</p><p>$\because l_{12}(x)&lt;\theta_{12} \therefore x\in\omega_2$</p><hr><p>例题：鱼类加工厂对鱼进行自动分类，$\omega_1$：鲈鱼；$\omega_2$：鲑鱼。模式特征$x=$长度。</p><p>已知：先验概率$P(\omega_1)=1/3,P(\omega_2)=1-P(\omega_1)=2/3$</p><p>$P(x=10|\omega_1=0.05),P(x=10|\omega_2=0.5)$</p><p>求：后验概率$P(\omega|x=10)$</p><p>解法1：利用Bayes公式</p><p>$\begin{aligned} P(\omega_1|x=10)&amp;=\dfrac{P(x=10|\omega_1)P(\omega_1)}{P(x=10)}\\ &amp;=\dfrac{P(x=10|\omega_1)P(\omega_1)}{P(x=10|\omega_1)P(\omega_1)+P(x=10|\omega_2)P(\omega_2)}\\ &amp;=\dfrac{0.05\times1/3}{0.05\times1/3+0.5\times2/3}\\ &amp;=0.048 \end{aligned}$</p><p>$P(\omega_2|x=10)=1-P(\omega_1|x=10)=0.952$</p><p>所以$\vec x\in\omega_2$，是鲑鱼</p><p>解法2：似然比形式</p><p>$l_{12}(x=10)=\dfrac{P(x=10|\omega_1)}{P(x=10|\omega_2)}=\dfrac{0.05}{0.5}=0.1$</p><p>判决阈值$\theta_{12}=\dfrac{P(\omega_2)}{P(\omega_1)}=\dfrac{2/3}{1/3}=2$</p><p>$l_{12}(x=10)&lt;\theta_{12}$，所以$\vec x\in\omega_2$，是鲑鱼</p><hr><h3 id="_4-2-最小损失准则判决" tabindex="-1">4.2 最小损失准则判决 <a class="header-anchor" href="#_4-2-最小损失准则判决" aria-hidden="true">#</a></h3><p>似然比形式</p><p>如果$\dfrac{P(\vec x|\omega_1)}{P(\vec x|\omega_2)}\gtrless\dfrac{P(\omega_2)(\lambda_{21}-\lambda_{22})}{P(\omega_1)(\lambda_{12}-\lambda_{11})}$，则判$\vec x\in\begin{cases} \omega_1\\ \omega_2\\ \end{cases}$</p><p>记似然比阈值$\theta_{12}=\dfrac{P(\omega_2)(\lambda_{21}-\lambda_{22})}{P(\omega_1)(\lambda_{12}-\lambda_{11})}$</p><p>则判决规则为：如果$l_{12}(\vec x)\gtrless\theta_{12}$，则判$\vec x\in\begin{cases} \omega_1\\ \omega_2\\ \end{cases}$</p><p>如果相等，称任判或拒判。</p><h4 id="定理" tabindex="-1">定理 <a class="header-anchor" href="#定理" aria-hidden="true">#</a></h4><p>使条件损失最小必然使总的平均损失最小、</p><p>当损失函数取0-1时最小损失准则等价于最小误判准则。</p><h2 id="第五章-决策树与随机森林" tabindex="-1">第五章 决策树与随机森林 <a class="header-anchor" href="#第五章-决策树与随机森林" aria-hidden="true">#</a></h2><h3 id="_5-1-决策树" tabindex="-1">5.1 决策树 <a class="header-anchor" href="#_5-1-决策树" aria-hidden="true">#</a></h3><h4 id="概念和特点" tabindex="-1">概念和特点 <a class="header-anchor" href="#概念和特点" aria-hidden="true">#</a></h4><ul><li>决策树是一种树形结构，其中每个内部节点表示在一个属性上的测试，每个分支代表一个测试输出，每个叶节点代表一种类别。</li><li>决策树学习时以实例为基础的归纳学习。</li><li>决策树采用的是自顶而下的递归方法。</li><li>决策树学习算法的最大优点是，他可以自学习，在学习的过程中不需要使用者了解过多背景知识，只需要对训练实例进行标注，就能进行学习。</li><li>属于有监督学习。从一类无序、无规则的十五中推理出决策树表示的分类规则。</li><li>建立决策树的关键，是在当前状态下选择哪些属性作为分类依据。</li><li>三种算法：ID3、C4.5、CART</li></ul><h4 id="对熵的理解" tabindex="-1">对熵的理解 <a class="header-anchor" href="#对熵的理解" aria-hidden="true">#</a></h4><p>熵是随机变量不确定性的度量，不确定性越大，熵值越大。若随机变量退化成定值，熵为0。同理，均匀分布是最不确定的分布。</p><p>熵定义了一个概率分布函数到一个值的映射。</p><h4 id="信息增益" tabindex="-1">信息增益 <a class="header-anchor" href="#信息增益" aria-hidden="true">#</a></h4><p>当熵和条件熵中的概率由数据估计得到时，所对应的熵和条件熵分别为经验熵和经验条件熵。 信息增益表示得到特征A的信息而使得类X的信息的不确定性减少的程度。</p><h4 id="特点" tabindex="-1">特点 <a class="header-anchor" href="#特点" aria-hidden="true">#</a></h4><p>决策树对训练数据有很好的分类能力，但对未知的测试数据未必有好的分类能力，泛化能力弱，即可能发生过拟合现象。</p><h4 id="bootstrap有放回抽样方法" tabindex="-1">bootstrap有放回抽样方法 <a class="header-anchor" href="#bootstrap有放回抽样方法" aria-hidden="true">#</a></h4><h3 id="随机森林" tabindex="-1">随机森林 <a class="header-anchor" href="#随机森林" aria-hidden="true">#</a></h3><p>随机森林在bagging基础上做了修改。</p><ul><li><p>从样本集中用Bootstrap采样选出n个样本；</p></li><li><p>从所有属性中随机选择k个属性，选择最佳分割属性作为节点建立CART决策树；</p></li><li><p>重复以上两步m次，即建立了m棵CART决策树</p></li><li><p>这m个CART形成随机森林，通过投票表决结果，决定数据属于哪一类</p></li></ul><h2 id="第六章-人工神经网络" tabindex="-1">第六章 人工神经网络 <a class="header-anchor" href="#第六章-人工神经网络" aria-hidden="true">#</a></h2><h3 id="人工神经网络的分类" tabindex="-1">人工神经网络的分类 <a class="header-anchor" href="#人工神经网络的分类" aria-hidden="true">#</a></h3><h4 id="从信息传递形式上" tabindex="-1">从信息传递形式上 <a class="header-anchor" href="#从信息传递形式上" aria-hidden="true">#</a></h4><ul><li>前向型：信息传递由后层神经元向前层神经元传递，从一层内的神经元之间没有信息交流。</li><li>反馈型：神经元之间不但互相作用，而且自身也有信息内耗。</li></ul><h4 id="按照神经元的学习过程" tabindex="-1">按照神经元的学习过程 <a class="header-anchor" href="#按照神经元的学习过程" aria-hidden="true">#</a></h4><ul><li>有指导学习网络</li><li>无指导学习网络</li></ul><h3 id="人工神经元模型的三个要素" tabindex="-1">人工神经元模型的三个要素 <a class="header-anchor" href="#人工神经元模型的三个要素" aria-hidden="true">#</a></h3><ol><li>一组连接，连接强度由各连接上的权值表示，权值为正表示激活，权值为负表示抑制，另有一个偏置值。</li><li>一个求和单元，用于求取个输入信号的加权和。</li><li>一个非线性的激活函数，起非线性映射的作用，并将神经元的输出幅度限制在一定范围内。</li></ol><h4 id="常用的激活函数" tabindex="-1">常用的激活函数 <a class="header-anchor" href="#常用的激活函数" aria-hidden="true">#</a></h4><p>硬极限函数、线性函数、对数S形函数、双曲正切S形函数</p><h3 id="特点-1" tabindex="-1">特点 <a class="header-anchor" href="#特点-1" aria-hidden="true">#</a></h3><p>当分类效果不好时，调整神经元数目等其他参数。函数非线性程度越高，对于BP网络要求越高，则相同的网络逼近效果要差一些，因曾神经元数目对于网络逼近效果也有一定影响，一般来说，隐层神经元数目越多，则BP网络逼近非线性函数的能力越强。</p><h2 id="第七章-深度学习" tabindex="-1">第七章 深度学习 <a class="header-anchor" href="#第七章-深度学习" aria-hidden="true">#</a></h2><h3 id="自动提取特征，学习特征" tabindex="-1">自动提取特征，学习特征 <a class="header-anchor" href="#自动提取特征，学习特征" aria-hidden="true">#</a></h3><p>机器学习中，获得好的特征是识别成功的关键</p><ul><li>高层的特征是低层特征的组合，从低层到高层的特征表示越来越抽象，越来越能表现语义或者意图</li><li>抽象层面越高，存在的可能猜测就越少，就越利于分类</li></ul><h3 id="浅层学习的局限" tabindex="-1">浅层学习的局限 <a class="header-anchor" href="#浅层学习的局限" aria-hidden="true">#</a></h3><p>人工神经网络(BP算法)：—虽被称作多层感知机，但实际是种只含有一层隐层节点的浅层模型</p><p>SVM、Boosting、最大熵方法（如LR，Logistic Regression）:带有一层隐层节点（如SVM、Boosting），或没有隐层节点（如LR）的浅层模型</p><p>局限性：有限样本和计算单元情况下对复杂函数的表示能力有限，针对复杂分类问题其泛化能力受限。</p><h3 id="深度学习好处" tabindex="-1">深度学习好处 <a class="header-anchor" href="#深度学习好处" aria-hidden="true">#</a></h3><p>可通过学习一种深层非线性网络结构，实现复杂函数逼近，表征输入数据分布式表示。</p><h3 id="深度学习vs神经网络" tabindex="-1">深度学习VS神经网络 <a class="header-anchor" href="#深度学习vs神经网络" aria-hidden="true">#</a></h3><p>相同点：二者均采用分层结构，系统包括输入层、隐层（多层）、输出层组成的多层网络</p><p>不同点：</p><ul><li>神经网络：采用BP算法调整参数，即采用迭代算法来训练整个网络。</li><li>深度学习：采用逐层训练机制。采用该机制的原因在于如果采用BP机制，对于一个deep network（7层以上），残差传播到最前面的层将变得很小，出现所谓的gradient diffusion（梯度扩散）。</li></ul><h4 id="神经网络的局限性：" tabindex="-1">神经网络的局限性： <a class="header-anchor" href="#神经网络的局限性：" aria-hidden="true">#</a></h4><ol><li>比较容易过拟合，参数比较难调整，而且需要不少技巧；</li><li>训练速度比较慢，在层次比较少（小于等于3）的情况下效果并不比其它方法更优</li></ol><h4 id="不采用bp算法的原因" tabindex="-1">不采用BP算法的原因 <a class="header-anchor" href="#不采用bp算法的原因" aria-hidden="true">#</a></h4><ol><li>反馈调整时，梯度越来越稀疏，从顶层越往下，误差校正信号越来越小；</li><li>收敛易至局部最小，</li><li>BP算法需要有标签数据来训练，但大部分数据是无标签的；</li></ol><h3 id="深度学习训练过程" tabindex="-1">深度学习训练过程 <a class="header-anchor" href="#深度学习训练过程" aria-hidden="true">#</a></h3><h4 id="第一步：采用自下而上的无监督学习" tabindex="-1">第一步：采用自下而上的无监督学习 <a class="header-anchor" href="#第一步：采用自下而上的无监督学习" aria-hidden="true">#</a></h4><ol><li>逐层构建单层神经元。</li><li>每层采用wake-sleep算法逐层调整。<br> 这个过程可以看作是一个feature learning的过程，是和传统神经网络区别最大的部分。</li></ol><h4 id="第二步：自顶向下的监督学习" tabindex="-1">第二步：自顶向下的监督学习 <a class="header-anchor" href="#第二步：自顶向下的监督学习" aria-hidden="true">#</a></h4><p>这一步是在第一步学习获得各层参数进的基础上，利用梯度下降法去微调整个网络参数。</p><p>深度学习的第一步实质上是一个网络参数初始化过程。深度学习模型是通过无监督学习输入数据的结构得到的，因而这个初值更接近全局最优，从而能够取得更好的效果。</p><h3 id="深度学习具体方法模型" tabindex="-1">深度学习具体方法模型 <a class="header-anchor" href="#深度学习具体方法模型" aria-hidden="true">#</a></h3><ul><li>自动编码器（ AutoEncoder ）</li><li>稀疏自动编码器(Sparse AutoEncoder)</li><li>降噪自动编码器(Denoising AutoEncoders)</li><li>深度信念网络（Deep Belief Net)</li><li>卷积神经网络（CNN）</li></ul><h3 id="卷积神经网络" tabindex="-1">卷积神经网络 <a class="header-anchor" href="#卷积神经网络" aria-hidden="true">#</a></h3><h4 id="cnn的关键技术" tabindex="-1">CNN的关键技术 <a class="header-anchor" href="#cnn的关键技术" aria-hidden="true">#</a></h4><p>局部感受野、权值共享、时间或空间子采样</p><h4 id="cnn的优点" tabindex="-1">CNN的优点 <a class="header-anchor" href="#cnn的优点" aria-hidden="true">#</a></h4><ul><li>隐式特征抽取</li><li>降低了网络的复杂性；</li><li>采用时间或空间子采样，有一定鲁棒性；</li><li>语音识别和图像处理方面有着独特优势。</li></ul><h4 id="cnn的缺点：" tabindex="-1">CNN的缺点： <a class="header-anchor" href="#cnn的缺点：" aria-hidden="true">#</a></h4><p>构建CNN模型需要大规模有标签数据；处理大尺寸图像耗时较长</p><h2 id="第八章-特征提取与选择" tabindex="-1">第八章 特征提取与选择 <a class="header-anchor" href="#第八章-特征提取与选择" aria-hidden="true">#</a></h2><h3 id="模式识别三大核心问题" tabindex="-1">模式识别三大核心问题 <a class="header-anchor" href="#模式识别三大核心问题" aria-hidden="true">#</a></h3><ul><li>特征数据采集</li><li>分类识别</li><li>特征提取与选择</li></ul><h3 id="特征提取的任务" tabindex="-1">特征提取的任务 <a class="header-anchor" href="#特征提取的任务" aria-hidden="true">#</a></h3><p>在得到实际对象的若干具体特征之后，再由这些原始特征产生出对分类识别最有效、数目最少的特征，</p><p>特征提取的目的是使在最小维数特征空间中类间距离较大，类内距离较小。</p><h3 id="选取特征的要求" tabindex="-1">选取特征的要求 <a class="header-anchor" href="#选取特征的要求" aria-hidden="true">#</a></h3><ol><li>具有很好的可分性。</li><li>具有可靠性。</li><li>尽可能强的独立性。</li><li>数量尽量少，同时损失的信息尽量小。</li></ol><h3 id="特征提取与特征选择的区别" tabindex="-1">特征提取与特征选择的区别 <a class="header-anchor" href="#特征提取与特征选择的区别" aria-hidden="true">#</a></h3><ol><li>特征选择：从L个度量值集合中按一定准则选出供分类用的子集，作为降维（m维，m&lt;L）的分类特征。</li><li>特征提取：使一组度量值L通过某种变换产生新的m个特征作为降维的分类特征，</li></ol><h3 id="特征提取与选择的方法" tabindex="-1">特征提取与选择的方法 <a class="header-anchor" href="#特征提取与选择的方法" aria-hidden="true">#</a></h3><p>直接选择法，变换法</p><h3 id="变换法里的离散k-l变换-dklt-主成分分析" tabindex="-1">变换法里的离散K-L变换(DKLT)主成分分析 <a class="header-anchor" href="#变换法里的离散k-l变换-dklt-主成分分析" aria-hidden="true">#</a></h3><p>有限离散K-L变换（DKLT）,是一种基于目标统计特性的最佳正交变换。</p><h4 id="dklt的性质" tabindex="-1">DKLT的性质 <a class="header-anchor" href="#dklt的性质" aria-hidden="true">#</a></h4><ul><li>使变换后产生的新的分量正交或不相关</li><li>以部分新分量表示原矢量均方误差最小</li><li>使变换矢量更趋确定、能量更趋集中</li></ul><p>取x的自相关阵Rx或协方差阵Cx的特征矢量矩阵的转置作为变换矩阵的变换称为离散K-L变换。</p> $$\vec x=T&#39;^{-1}\vec y=T\vec y=\sum_{i=1}^{n}y_i\vec t_i$$ <h4 id="离散k-l展开式" tabindex="-1">离散K-L展开式 <a class="header-anchor" href="#离散k-l展开式" aria-hidden="true">#</a></h4><p>$\lambda_i(R_{\vec x})\geqslant\lambda_i(C_{\vec x})$</p><p>这表明对于相同的m，第一种估计式比第二种估计式的均方差大。</p><h4 id="步骤：" tabindex="-1">步骤： <a class="header-anchor" href="#步骤：" aria-hidden="true">#</a></h4><ol><li>求样本集{X}的总体自相关矩阵R或协方差矩阵C。</li><li>求$R$或$C$的特征值$\lambda_j,j=1,2,\dots,n$。对特征值从大到小排序，选择前$d$个较大的特征值。</li><li>计算$d$个特征值对应的特征向量$\vec u_j,j=1,2,\dots,d$，构成变换矩阵$U$。</li><li>对$\{X\}$中的每个$X$进行K-L变换，得到变换后的向量$X^*$，$X^*=U^{\rm T}X$</li></ol><h2 id="第九章-句法模式识别" tabindex="-1">第九章 句法模式识别 <a class="header-anchor" href="#第九章-句法模式识别" aria-hidden="true">#</a></h2><p>汉字、字符、语言、图像、生物的识别</p><h3 id="定义" tabindex="-1">定义 <a class="header-anchor" href="#定义" aria-hidden="true">#</a></h3><p>以结构基元为基础，利用模式的结构信息完成分类的过程。也称为句法模式识别。</p><h4 id="基元" tabindex="-1">基元 <a class="header-anchor" href="#基元" aria-hidden="true">#</a></h4><p>指构成模式结构信息的基本单元，本身不包含有意义的结构信息。</p><h4 id="理论基础形式" tabindex="-1">理论基础形式 <a class="header-anchor" href="#理论基础形式" aria-hidden="true">#</a></h4><p>语言</p><h4 id="模式描述方法" tabindex="-1">模式描述方法 <a class="header-anchor" href="#模式描述方法" aria-hidden="true">#</a></h4><p>符号串，树，图</p><h4 id="模式判定" tabindex="-1">模式判定 <a class="header-anchor" href="#模式判定" aria-hidden="true">#</a></h4><p>用一个文法表示一个类，m类就有m个文法，然后判定未知模式遵循哪一个文法。</p><p>在学习过程中，确定基元与基元之间的关系，推断出生成景物的方法。</p><p>判决过程中，提取基元，基元连接关系，句法分析。判断类型。</p><h3 id="句法模式识别的特点" tabindex="-1">句法模式识别的特点 <a class="header-anchor" href="#句法模式识别的特点" aria-hidden="true">#</a></h3><ul><li>结构模式识别是与统计模式识，一个基于结构信息，一个基于特征值</li><li>结构模式识别可以得到每个模式的结构性质</li><li>结构模式识别的依据是模式间结构上的“相似性”</li><li>结构模式识别用小而简单的基元与语法规则描述和识别</li><li>大而复杂的模式，通过对基元的识别，进而识别子模式，最终识别复杂模式。</li></ul><h3 id="与自然语言对比" tabindex="-1">与自然语言对比 <a class="header-anchor" href="#与自然语言对比" aria-hidden="true">#</a></h3><p>模式$\leftrightarrow$句子</p><p>子模式$\leftrightarrow$词组</p><p>基元$\leftrightarrow$单词</p><p>组合关系$\leftrightarrow$自然语言的文法</p><p>符合某个文法的所有句子的集合$\leftrightarrow$一个模式类</p><h3 id="句法" tabindex="-1">句法 <a class="header-anchor" href="#句法" aria-hidden="true">#</a></h3><ul><li>句法是指由字（词）构成句子的方式，也就是一个句子组成的规则。</li><li>句法具有递归性</li><li>用句法来表达基元间的结构关系。</li></ul><h3 id="文法-类" tabindex="-1">文法(类) <a class="header-anchor" href="#文法-类" aria-hidden="true">#</a></h3><ul><li>文法是指一类相似的句子的共同句法规则。</li><li>可以用文法来表示一类样本的共同特点。</li><li>对某个具体的句子进行句法分析，判别与某类的文法是否相似，可以实现模式识别。</li></ul><h4 id="文法推断" tabindex="-1">文法推断 <a class="header-anchor" href="#文法推断" aria-hidden="true">#</a></h4><p>用已知类别的模式样本集训练类别文法的过程</p><h4 id="句法分析" tabindex="-1">句法分析 <a class="header-anchor" href="#句法分析" aria-hidden="true">#</a></h4><p>利用文法对未知类别的句法模式进行识别或分类的过程。</p><h3 id="字母表，句子，语言，文法" tabindex="-1">字母表，句子，语言，文法 <a class="header-anchor" href="#字母表，句子，语言，文法" aria-hidden="true">#</a></h3><ul><li>$V^*$：V中符号组成的所有句子的集合，包括空句；</li><li>$V^+$：不包含空句的句子集合。</li><li>语言：由字母表中的符号组成的句子集合，用L表示</li><li>文法：构成一种语言的句子所必须遵守的规则。是一个四元式，由四个参数构成：</li><li>$V_N$：非终止符的有限集，子模式的集合，大写字母表示。</li><li>$V_T$：终止符有限集，基元的集合，字母表起始部分的小写字母表示 。</li><li>P：产生式的有限集。用文法产生句子时的重写规则。</li><li>S：起始符，代表模式本身，特殊的非终止符。用产生式构成句子时，必须由左边是S的产生式开始。</li></ul><h3 id="文法分类" tabindex="-1">文法分类 <a class="header-anchor" href="#文法分类" aria-hidden="true">#</a></h3><p>0型文法、1型文法、2型文法和3型文法。</p></div></div><div class="content nav"><span><a href="/posts/love2d-require-c-library.html"><i class="fa fa-angle-left"></i> 关于love2d引擎require导入C/C++编写的.dll/.so扩展库问题</a></span><span><a href="/posts/pyside2-quick-start.html">pyside2 快速入门 <i class="fa fa-angle-right"></i></a></span></div><div id="waline"></div><div class="toc"><ol><!--[--><li class="h2 active"><a href="#第三章-判别域代数界面方程">第三章 判别域代数界面方程</a></li><li class="h3"><a href="#_3-1-用判别域界面方程分类的概念">3.1 用判别域界面方程分类的概念</a></li><li class="h3"><a href="#_3-2-线性判别函数">3.2 线性判别函数</a></li><li class="h3"><a href="#_3-3-判别函数值的鉴别意义、权空间及解空间">3.3 判别函数值的鉴别意义、权空间及解空间</a></li><li class="h3"><a href="#_3-4-fisher线性判别">3.4 Fisher线性判别</a></li><li class="h3"><a href="#_3-5-感知器算法">3.5 感知器算法</a></li><li class="h3"><a href="#_3-6-一般情况下的判别函数权矢量算法">3.6 一般情况下的判别函数权矢量算法</a></li><li class="h3"><a href="#_3-7-广义线性判别函数">3.7 广义线性判别函数</a></li><li class="h3"><a href="#_3-8-二次判别函数">3.8 二次判别函数</a></li><li class="h3"><a href="#_3-9-支持向量机">3.9 支持向量机</a></li><li class="h2"><a href="#第四章-统计判别">第四章 统计判别</a></li><li class="h3"><a href="#_4-0-前提条件">4.0 前提条件</a></li><li class="h3"><a href="#_4-1-最小误判概率判决">4.1 最小误判概率判决</a></li><li class="h3"><a href="#_4-2-最小损失准则判决">4.2 最小损失准则判决</a></li><li class="h2"><a href="#第五章-决策树与随机森林">第五章 决策树与随机森林</a></li><li class="h3"><a href="#_5-1-决策树">5.1 决策树</a></li><li class="h3"><a href="#随机森林">随机森林</a></li><li class="h2"><a href="#第六章-人工神经网络">第六章 人工神经网络</a></li><li class="h3"><a href="#人工神经网络的分类">人工神经网络的分类</a></li><li class="h3"><a href="#人工神经元模型的三个要素">人工神经元模型的三个要素</a></li><li class="h3"><a href="#特点-1">特点</a></li><li class="h2"><a href="#第七章-深度学习">第七章 深度学习</a></li><li class="h3"><a href="#自动提取特征，学习特征">自动提取特征，学习特征</a></li><li class="h3"><a href="#浅层学习的局限">浅层学习的局限</a></li><li class="h3"><a href="#深度学习好处">深度学习好处</a></li><li class="h3"><a href="#深度学习vs神经网络">深度学习VS神经网络</a></li><li class="h3"><a href="#深度学习训练过程">深度学习训练过程</a></li><li class="h3"><a href="#深度学习具体方法模型">深度学习具体方法模型</a></li><li class="h3"><a href="#卷积神经网络">卷积神经网络</a></li><li class="h2"><a href="#第八章-特征提取与选择">第八章 特征提取与选择</a></li><li class="h3"><a href="#模式识别三大核心问题">模式识别三大核心问题</a></li><li class="h3"><a href="#特征提取的任务">特征提取的任务</a></li><li class="h3"><a href="#选取特征的要求">选取特征的要求</a></li><li class="h3"><a href="#特征提取与特征选择的区别">特征提取与特征选择的区别</a></li><li class="h3"><a href="#特征提取与选择的方法">特征提取与选择的方法</a></li><li class="h3"><a href="#变换法里的离散k-l变换-dklt-主成分分析">变换法里的离散K-L变换(DKLT)主成分分析</a></li><li class="h2"><a href="#第九章-句法模式识别">第九章 句法模式识别</a></li><li class="h3"><a href="#定义">定义</a></li><li class="h3"><a href="#句法模式识别的特点">句法模式识别的特点</a></li><li class="h3"><a href="#与自然语言对比">与自然语言对比</a></li><li class="h3"><a href="#句法">句法</a></li><li class="h3"><a href="#文法-类">文法(类)</a></li><li class="h3"><a href="#字母表，句子，语言，文法">字母表，句子，语言，文法</a></li><li class="h3"><a href="#文法分类">文法分类</a></li><!--]--></ol></div></div><!--]--></main><!--]--></div>
    <script>__VP_HASH_MAP__ = JSON.parse("{\"index.md\":\"da37999e\",\"posts_algorithm-2048.md\":\"6a2b0a99\",\"posts_ascii-art.md\":\"80431cea\",\"posts_calc-demo.md\":\"e2c19a19\",\"posts_call-cpp-from-python.md\":\"77a830d6\",\"posts_cpp-promise.md\":\"31e25376\",\"posts_d3-note.md\":\"7d872a97\",\"posts_fix-matplotlib-chinese.md\":\"a3961590\",\"posts_indexeddb.md\":\"4b5c2177\",\"posts_love2d-require-c-library.md\":\"b8c7e2db\",\"posts_pattern-recognition-note.md\":\"f88ce139\",\"posts_pyside2-quick-start.md\":\"f2fdb430\",\"posts_python-opencv.md\":\"f76e7ea5\",\"posts_ripple-animation.md\":\"bef572d8\",\"posts_rust-list-slice.md\":\"adb1c9de\",\"posts_tensorflow-note.md\":\"9c66e17f\",\"posts_tetris-field-compress.md\":\"afd684a8\",\"posts_write-code-from-answer.md\":\"99e5f7c3\",\"posts_xjb-utils.md\":\"ef2363ca\",\"readme.md\":\"d3c39a50\",\"tags_index.md\":\"6e66c6ee\"}")</script>
    <script type="module" async src="/assets/app.d81b9fb5.js"></script>
    
  </body>
</html>